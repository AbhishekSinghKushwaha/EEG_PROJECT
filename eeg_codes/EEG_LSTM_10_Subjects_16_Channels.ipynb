{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.12.4)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Dropout, Flatten, Reshape, LSTM, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyedflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyedflib\n",
    "import numpy as np\n",
    "\n",
    "def _read_py_function(filename, num_channels=16):\n",
    "    # Open the EDF file\n",
    "    f = pyedflib.EdfReader(filename)\n",
    "    \n",
    "    # Get the number of channels and the signal labels\n",
    "    n_channels = f.signals_in_file\n",
    "    print(f\"Total channels in file: {n_channels}\")\n",
    "    \n",
    "    # Ensure that we are fetching only the desired number of channels (16 in this case)\n",
    "    if num_channels > n_channels:\n",
    "        raise ValueError(f\"The file contains only {n_channels} channels, but {num_channels} were requested.\")\n",
    "    \n",
    "    # Initialize eeg_data to store only the first `num_channels`\n",
    "    eeg_data = np.zeros((num_channels, f.getNSamples()[0]), dtype=np.float32)\n",
    "    \n",
    "    # Read only the first `num_channels` channels\n",
    "    for i in range(num_channels):\n",
    "        eeg_data[i, :] = f.readSignal(i)\n",
    "\n",
    "    n_samples = f.getNSamples()[0]\n",
    "    reminder = int(n_samples % 160)\n",
    "    \n",
    "    # Print statement to check values\n",
    "    print(f\"Original n_samples: {n_samples}\")\n",
    "    print(f\"Reminder: {reminder}\")\n",
    "    \n",
    "    n_samples -= reminder\n",
    "    seconds = int(n_samples / 160)\n",
    "    \n",
    "    # Extract person_id from the filename\n",
    "    path = filename.split(\"\\\\\")\n",
    "    person_id = int(path[-1].partition(\"S\")[2].partition(\"R\")[0])\n",
    "    \n",
    "    # Create one-hot encoded labels\n",
    "    label = np.zeros(10, dtype=bool)\n",
    "    label[person_id-1] = 1\n",
    "    labels = np.tile(label, (seconds, 1))\n",
    "    \n",
    "    # Normalization step\n",
    "    for i in range(num_channels):\n",
    "        channel_data = eeg_data[i, :]\n",
    "        mean_i = np.mean(channel_data)  # Compute mean of channel i\n",
    "        std_i = np.std(channel_data)    # Compute standard deviation of channel i\n",
    "        \n",
    "        if std_i != 0:  # Avoid division by zero\n",
    "            eeg_data[i, :] = (channel_data - mean_i) / std_i  # Normalize channel i\n",
    "\n",
    "    # Transpose the data to shape (n_samples, n_channels)\n",
    "    eeg_data = eeg_data.T\n",
    "    \n",
    "    if reminder > 0:\n",
    "        eeg_data = eeg_data[:-reminder, :]\n",
    "    \n",
    "    # Split the data into 160-sample chunks\n",
    "    intervals = np.linspace(0, n_samples, num=seconds, endpoint=False, dtype=int)\n",
    "    eeg_data = np.split(eeg_data, intervals)\n",
    "    del eeg_data[0]\n",
    "    eeg_data = np.array(eeg_data)\n",
    "    \n",
    "    return eeg_data, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_eeg_data_with_pyedflib(data_path, max_subjects=10):\n",
    "    # Initialize the global variables\n",
    "    global subject_data, all_train_data, all_train_labels, all_test_data, all_test_labels\n",
    "    global train_data, val_data, train_labels, val_labels\n",
    "\n",
    "    # Initialize the variables before using them\n",
    "    subject_data = {}\n",
    "    all_train_data = []\n",
    "    all_train_labels = []\n",
    "    all_test_data = []\n",
    "    all_test_labels = []\n",
    "\n",
    "    subject_count = 0  # Counter to track the number of subjects processed\n",
    "\n",
    "    for subject_dir in os.listdir(data_path):\n",
    "        subject_path = os.path.join(data_path, subject_dir)\n",
    "        if os.path.isdir(subject_path):\n",
    "            print(f\"Processing subject: {subject_dir}\")\n",
    "            subject_data[subject_dir] = {}\n",
    "            for recording_file in os.listdir(subject_path):\n",
    "                recording_path = os.path.join(subject_path, recording_file)\n",
    "                if recording_file.endswith('.edf'):\n",
    "                    recording_id = os.path.splitext(recording_file)[0]\n",
    "                    print(f\"Loading recording: {recording_file}\")\n",
    "\n",
    "                    try:\n",
    "                        #eeg_data, labels = _read_py_function(recording_path)\n",
    "                        eeg_data, labels = _read_py_function(recording_path)\n",
    "\n",
    "                        print(f\"Data shape after processing: {eeg_data.shape}\")\n",
    "                        print(f\"Labels shape: {labels.shape}\")\n",
    "\n",
    "                        train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "                            eeg_data, labels, test_size=0.1, random_state=42\n",
    "                        )\n",
    "                        \n",
    "                        all_train_data.append(train_data)\n",
    "                        all_train_labels.append(train_labels)\n",
    "                        all_test_data.append(test_data)\n",
    "                        all_test_labels.append(test_labels)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading {recording_file}: {e}\")\n",
    "\n",
    "            subject_count += 1  # Increment the subject counter\n",
    "\n",
    "            # Stop processing once the desired number of subjects is reached\n",
    "            if subject_count >= max_subjects:\n",
    "                print(f\"Processed {max_subjects} subjects. Stopping further processing.\")\n",
    "                break\n",
    "\n",
    "    print(f\"Train data list length: {len(all_train_data)}\")\n",
    "    print(f\"Train labels list length: {len(all_train_labels)}\")\n",
    "    print(f\"Test data list length: {len(all_test_data)}\")\n",
    "    print(f\"Test labels list length: {len(all_test_labels)}\")\n",
    "\n",
    "    if all_train_data and all_train_labels:\n",
    "        all_train_data = np.concatenate(all_train_data, axis=0)\n",
    "        all_train_labels = np.concatenate(all_train_labels, axis=0)\n",
    "    else:\n",
    "        print(\"No training data loaded.\")\n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "    if all_test_data and all_test_labels:\n",
    "        all_test_data = np.concatenate(all_test_data, axis=0)\n",
    "        all_test_labels = np.concatenate(all_test_labels, axis=0)\n",
    "    else:\n",
    "        print(\"No test data loaded.\")\n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "    if len(all_train_data) == 0 or len(all_train_labels) == 0:\n",
    "        print(\"No data available for splitting into training and validation sets.\")\n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "    # Split train data further into training and validation sets\n",
    "    train_data, val_data, train_labels, val_labels = train_test_split(\n",
    "        all_train_data, all_train_labels, test_size=0.25, random_state=42\n",
    "    )\n",
    "\n",
    "    return train_data, train_labels, val_data, val_labels, all_test_data, all_test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r\"C:\\Users\\akhileshsing2024\\Downloads\\eeg-motor-movementimagery-dataset-1.0.0\\files\"\n",
    "train_data, train_labels, val_data, val_labels, test_data, test_labels = load_eeg_data_with_pyedflib(data_path)\n",
    "# Access global variables directly if needed\n",
    "print(f\"Train data shape: {train_data.shape if train_data is not None else 'None'}\")\n",
    "print(f\"Validation data shape: {val_data.shape if val_data is not None else 'None'}\")\n",
    "print(f\"Test data shape: {test_data.shape if test_data is not None else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM:\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def eeg_biometric_identification_model_lstm(input_shape, n_classes, lstm_size=192, keep_prob=0.5):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    # Initial LSTM Layers for feature extraction (replacing Conv1D layers)\n",
    "    lstm1 = tf.keras.layers.LSTM(units=128, return_sequences=True)(inputs)  # First LSTM layer\n",
    "    lstm2 = tf.keras.layers.LSTM(units=256, return_sequences=True)(lstm1)   # Second LSTM layer\n",
    "    lstm3 = tf.keras.layers.LSTM(units=512, return_sequences=True)(lstm2)   # Third LSTM layer\n",
    "    lstm4 = tf.keras.layers.LSTM(units=1024, return_sequences=False)(lstm3) # Fourth LSTM layer\n",
    "\n",
    "    # Fully connected layer with 192 neurons and dropout\n",
    "    fc1 = tf.keras.layers.Dense(units=192, activation='relu')(lstm4)\n",
    "    dropout_fc1 = tf.keras.layers.Dropout(rate=1 - keep_prob)(fc1)\n",
    "\n",
    "    # Reshape to (batch_size, timesteps=1, features=192) for additional LSTM layers\n",
    "    lstm_input = tf.keras.layers.Reshape((1, 192))(dropout_fc1)\n",
    "\n",
    "    # Add the two specific LSTM layers from the original architecture\n",
    "    lstm_out = tf.keras.layers.LSTM(units=lstm_size, return_sequences=True)(lstm_input)  # First LSTM layer\n",
    "    lstm_out = tf.keras.layers.LSTM(units=lstm_size, return_sequences=False)(lstm_out)  # Second LSTM layer\n",
    "\n",
    "    # Fully connected layers after LSTM\n",
    "    fc2 = tf.keras.layers.Dense(units=192, activation='relu')(lstm_out)\n",
    "    fc3 = tf.keras.layers.Dense(units=192, activation='relu')(fc2)\n",
    "\n",
    "    # Output layer (Softmax for classification)\n",
    "    output = tf.keras.layers.Dense(n_classes, activation='softmax')(fc3)\n",
    "\n",
    "    # Create the model\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "input_shape = (160, 64)  # 160 samples, 64 channels (1-second EEG recording)\n",
    "n_classes = 10  # Number of subjects\n",
    "keep_prob = 0.5  # Dropout rate\n",
    "\n",
    "model = eeg_biometric_identification_model_lstm(input_shape, n_classes, lstm_size=192, keep_prob=keep_prob)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"C:/Users/akhileshsing2024/Desktop/Epochs/best_model/LSTM_16.weights.h5\"  # Save the best model in the current directory\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 save_best_only=True,  # Save only the best model\n",
    "                                                 monitor='val_loss',  # Monitor validation loss\n",
    "                                                 mode='min',  # Save the model with the lowest validation loss\n",
    "                                                 verbose=1)\n",
    "\n",
    "# Training the model\n",
    "with tf.device('/GPU:0'):  # Ensure training uses the GPU\n",
    "    history = model.fit(train_data, train_labels, \n",
    "                        epochs=20, \n",
    "                        validation_data=(val_data, val_labels), \n",
    "                        batch_size=80,  \n",
    "                        callbacks=[cp_callback]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the training history as a numpy file\n",
    "np.save(\"history_LSTM_16.npy\", history.history)\n",
    "\n",
    "# Load the training history\n",
    "history = np.load(\"history_LSTM_16.npy\", allow_pickle=True).item()\n",
    "\n",
    "# Ensure history is a dictionary\n",
    "if type(history) is not dict:\n",
    "    history = history.history\n",
    "\n",
    "# Find the epoch with the highest validation accuracy\n",
    "max_value = max(history['val_accuracy'])\n",
    "print(f\"Max Validation Accuracy: {max_value}\")\n",
    "\n",
    "max_index = history['val_accuracy'].index(max_value)\n",
    "print(f\"Best Epoch Index: {max_index}\")\n",
    "print(f\"Corresponding Training Accuracy: {history['accuracy'][max_index]}\")\n",
    "\n",
    "# Load the best checkpoint based on validation accuracy\n",
    "model.load_weights(checkpoint_path)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_accuracy = model.evaluate(x=test_data, y=test_labels)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "#plt.savefig(\"accuracy_plot.png\")  # Save the accuracy plot\n",
    "plt.show()\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(history['loss'], label='Training Loss')\n",
    "plt.plot(history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "#plt.savefig(\"loss_plot.png\")  # Save the loss plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "\n",
    "# Get the predictions from the model on the test data\n",
    "predictions = model.predict(test_data)\n",
    "\n",
    "# Convert the predicted probabilities into class predictions (argmax over class probabilities)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Convert the one-hot encoded test labels back to their original class labels\n",
    "true_classes = np.argmax(test_labels, axis=1)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(true_classes, predicted_classes)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "# Plot the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.arange(n_classes))\n",
    "disp.plot(cmap='Blues', values_format='d')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
